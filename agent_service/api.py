import os
import json
import datetime
from typing import Dict, Any

# --- AdiÃ§Ãµes para AutomaÃ§Ã£o e Frontend ---
from dotenv import load_dotenv
from fastapi.staticfiles import StaticFiles
from starlette.responses import FileResponse 
from fastapi.responses import HTMLResponse
# ----------------------------------------

# --- Carregar VariÃ¡veis de Ambiente ---
load_dotenv() 

# --- ImportaÃ§Ãµes do FastAPI e Pydantic ---
from fastapi import FastAPI
from pydantic import BaseModel, Field

# --- ImportaÃ§Ãµes do Firebase ---
import firebase_admin
from firebase_admin import credentials
from firebase_admin import firestore

# --- ImportaÃ§Ãµes do Gemini API ---
from google import genai
from google.genai import types


# --- ConfiguraÃ§Ã£o do LLM ---
try:
    if os.environ.get("GEMINI_API_KEY"):
        LLM_CLIENT = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
        print("ðŸ¤– Cliente Gemini API inicializado.")
    else:
        LLM_CLIENT = None
        print("âš ï¸ VariÃ¡vel GEMINI_API_KEY nÃ£o encontrada. Usando simulaÃ§Ã£o antiga.")
except Exception as e:
    print(f"âŒ Erro ao inicializar o Cliente Gemini: {e}")
    LLM_CLIENT = None

# --- ConfiguraÃ§Ã£o do Firebase ---
try:
    # Verifica se jÃ¡ existe um app inicializado para evitar erro de duplicidade no reload
    if not firebase_admin._apps:
        cred = credentials.ApplicationDefault()
        firebase_admin.initialize_app(cred)
    db = firestore.client()
    print("ðŸš€ Firebase e Firestore inicializados com sucesso.")
except Exception as e:
    print(f"âš ï¸ Erro ao inicializar o Firebase: {e}")
    db = None

app = FastAPI(title="Customer Feedback AI Agent")

# --- BLOCO: MONTAR ARQUIVOS ESTÃTICOS ---
app.mount("/static", StaticFiles(directory="static"), name="static")
# ---------------------------------------------


# --- Modelos Pydantic ---
class FeedbackRequest(BaseModel):
    text: str

class FeedbackResponse(BaseModel):
    category: str = Field(description="Classifique em uma destas opÃ§Ãµes: 'LogÃ­stica', 'Bug no App', 'SugestÃ£o', 'Atendimento' ou 'Geral'.")
    sentiment: str = Field(description="Classifique em 'Positivo', 'Neutro' ou 'Negativo'.")
    summary: str = Field(description="Uma breve descriÃ§Ã£o da aÃ§Ã£o necessÃ¡ria (se for Negativo) ou o ponto positivo (se for Positivo).")

# --- FunÃ§Ã£o de Fallback (LÃ³gica Antiga de Keywords) ---
def simular_processamento_antigo(text: str) -> FeedbackResponse:
    """ Usa a lÃ³gica de keywords como fallback. """
    text_lower = text.lower()
    
    if "atrasou" in text_lower or "chegou tarde" in text_lower or "logÃ­stica" in text_lower:
        ml_category = "LogÃ­stica"
    elif "travou" in text_lower or "bug" in text_lower or "pix" in text_lower:
        ml_category = "Bug no App"
    elif "sugestÃ£o" in text_lower or "adicionar" in text_lower:
        ml_category = "SugestÃ£o"
    else:
        ml_category = "Geral / Outros"

    if "rude" in text_lower or "atrasou" in text_lower or "fria" in text_lower or "errada" in text_lower or "sumiu" in text_lower:
        sentiment = "Negativo"
        summary = "Cliente insatisfeito. Problemas identificados: " + ml_category + ". Requer atenÃ§Ã£o imediata da equipe responsÃ¡vel."
    
    elif "gentil" in text_lower or "quente" in text_lower or "5 estrelas" in text_lower or "impecÃ¡vel" in text_lower or "adorei" in text_lower:
        sentiment = "Positivo"
        summary = "Cliente muito satisfeito! Feedback positivo sobre " + ml_category + ". Oportunidade para incentivar boas prÃ¡ticas."
    
    else:
        sentiment = "Neutro"
        summary = "O feedback nÃ£o contÃ©m palavras-chave crÃ­ticas. Classificado como " + ml_category + " para anÃ¡lise de produto."
        
    return FeedbackResponse(
        category=ml_category,
        sentiment=sentiment,
        summary=summary
    )

# --- FunÃ§Ã£o para Salvar no Firestore ---
def save_to_firestore(data: Dict[str, Any]):
    """ Salva os dados de feedback processados na coleÃ§Ã£o 'feedback_history' do Firestore. """
    if db:
        try:
            data['timestamp'] = datetime.datetime.now(tz=datetime.timezone.utc)
            db.collection('feedback_history').add(data)
            print(f"âœ… Feedback salvo no Firestore: {data.get('category')} / {data.get('sentiment')}")
        except Exception as e:
            print(f"âŒ Erro ao salvar no Firestore: {e}")
    else:
        print("âŒ Firestore nÃ£o inicializado. NÃ£o foi possÃ­vel salvar o feedback.")

# --- Endpoint Principal (com LLM) ---
@app.post("/process", response_model=FeedbackResponse)
def process_feedback(request: FeedbackRequest):
    """
    Endpoint que envia o feedback para o LLM (Gemini 2.5 Flash)
    para classificaÃ§Ã£o estruturada e salva no Firestore.
    """
    text = request.text
    
    if LLM_CLIENT is None:
        print("âš ï¸ Usando Fallback. LLM_CLIENT Ã© None.")
        return simular_processamento_antigo(text)

    # --- 1. PROMPT ENGINEERING e CONFIGURAÃ‡ÃƒO ---
    prompt = f"Analise o texto do cliente e retorne o objeto JSON com base no esquema fornecido. TEXTO: '{text}'"

    # --- CORREÃ‡ÃƒO APLICADA AQUI ---
    # Removemos a conversÃ£o manual para types.Schema e passamos a classe Pydantic direto.
    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=FeedbackResponse # O SDK lida com a conversÃ£o automaticamente
    )

    # --- 2. CHAMADA AO LLM ---
    try:
        response = LLM_CLIENT.models.generate_content(
            model='gemini-2.0-flash', # Ajuste para o modelo correto disponivel (2.0 flash Ã© o padrÃ£o atual)
            contents=prompt,
            config=config
        )
        
        # Parsing da resposta
        # O modelo pode retornar o objeto jÃ¡ parseado em response.parsed se suportado,
        # mas manteremos a lÃ³gica de json.loads para garantir compatibilidade com o texto cru.
        llm_json_string = response.text.strip()
        
        try:
            llm_output_data = json.loads(llm_json_string)
        except json.JSONDecodeError:
            print(f"âŒ ERRO DE PARSING: O LLM retornou JSON invÃ¡lido.")
            print(f"âŒ TEXTO RETORNADO PELO LLM: {llm_json_string}")
            raise ValueError("Resposta do LLM nÃ£o Ã© um JSON vÃ¡lido.")

        response_data = FeedbackResponse(**llm_output_data)

    except Exception as e:
        print(f"âŒ Erro na chamada do LLM/Processamento: {e}")
        summary_text = str(e)
        if len(summary_text) > 100:
             summary_text = summary_text[:100] + "..."
             
        response_data = FeedbackResponse(
             category="Erro de LLM", 
             sentiment="Neutro", 
             summary=f"Falha na API Gemini: {summary_text}"
        )

    # --- 3. SALVAR NO FIRESTORE ---
    firestore_data = response_data.model_dump()
    firestore_data["raw_text"] = text
    save_to_firestore(firestore_data)
        
    return response_data


# --- ENDPOINT PARA SERVIR O FRONTEND ---
@app.get("/", response_class=HTMLResponse, include_in_schema=False)
async def serve_frontend():
    """Serve a pÃ¡gina HTML principal do frontend (index.html)."""
    # Verifica se o arquivo existe antes de tentar servir
    if os.path.exists("static/index.html"):
        return FileResponse("static/index.html")
    return HTMLResponse(content="<h1>Frontend nÃ£o encontrado em /static/index.html</h1>", status_code=404)

# --- Endpoint health_check ---
@app.get("/health_check")
def health_check():
    return {"status": "Agente Ativo ðŸš€"}